---
title: "LOD score"
author: |
  | Deependra Dhakal
  | Gokuleshwor Agriculture and Animal Science College
  | Tribhuwan University
  | \textit{ddhakal.rookie@gmail.com}
  | \url{https://rookie.rbind.io}
date: Academic year 2019-2020
output: 
  binb::monash:
    toc: true
    keep_tex: true
    slide_level: 2
    includes:
      in_header: beamer_header_template.tex
colortheme: monashwhite
outertheme: smoothtree
innertheme: rounded
innercolortheme: rose
outercolortheme: whale
linkcolor: red
urlcolor: lightgrayd
# titlefontsize: 22pt # breaks custom \lineskip
fontsize: 11pt
classoption: "aspectratio=169" # "compressed"
bibliography: [./../bibliographies.bib]
header-includes: 
  - \AtBeginSubsection{}
---

```{r setup, include=FALSE}
library(knitr)
require(tidyverse)
require(broom)
require(broom.mixed)
require(broomExtra)
set.seed(453)
# invalidate cache when the package version changes
knitr::opts_chunk$set(tidy = FALSE, echo = FALSE, 
                  message = FALSE, warning = FALSE,
                  out.width = "45%", cache = TRUE, 
                  dev.args=list(bg=grey(0.9), pointsize=11))
options(knitr.table.format = "latex")
options(knitr.kable.NA = "", digits = 2)
options(kableExtra.latex.load_packages = FALSE)
theme_set(theme_bw())
```

# LOD score

##

- Genetic markers located on the same chromosome tend to remain together during sexual reproduction (linkage groups). 
- That is, they do not exhibit independent assortment. Consequently, there are as many linkage groups as there are homologous pairs of chromosomes.
- Manual linkage analysis is feasible if only a few markers are being studied. 
- Modern linkage map construction is a computerized operation, feasible through mapping software packages. - These computer software packages use the coded information from the segregating population to determine recombination frequencies. 
- The basic calculation is a ratio (odds ratio) of linkage versus no linkage, expressed as the log of the ratio (logarithm of odds or LOD).

##

- The LOD score compares the likelihood of obtaining the test data if the two loci are indeed linked, to the likelihood of observing the same data purely by chance.
- A **LOD value** or score measures the likelihood of linkage between two markers, a score of more than three usually being the cut-off minimum for mapping. 
- Positive LOD scores favour the presence of linkage, whereas negative LOD scores indicate that linkage is less likely. 
- A LOD of three indicates a 1000:1 odds in favor of genetic linkage (linkage between the two markers is a thousand times more likely than no linkage). 
- The researcher may vary the stringency of mapping by, for example, lowering the LOD score to detect a greater level of linkage.

##

Linear regression models are defined by the equation 

$$
Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_qx_q + \epsilon; \epsilon \sim N(0, \sigma^2)
$$

which calculates the response $Y$ directly. Logisitc regression is defined in a similar manner,

Consider a model with two predictors, ${\displaystyle x_{1}}$ and ${\displaystyle x_{2}}$, and one binary (Bernoulli) response variable ${\displaystyle Y}$, which we denote ${\displaystyle p=P(Y=1)}$. We assume a linear relationship between the predictor variables, and the log-odds of the event that ${\displaystyle Y=1}$.

This linear relationship can be written in the following mathematical form (where $\ell$ is the log-odds, $b$ is the base of the logarithm, and ${\displaystyle \beta _{i}}$ are parameters of the model):

$$
{\displaystyle \ell =\log _{b}\left({\frac {p(\mathbf{x})}{1-p(\mathbf{x})}}\right)=\beta _{0}+\beta _{1}x_{1}+...+\beta _{p-1}x_{p-1}}
$$

##

- In binary context, the odds are probability for a 'positive' event ($Y = 1$) divided by the probability of 'negative' event ($Y = 0$).

$$
\frac{p(\mathbf{x})}{1-p(\mathbf{x})} = \frac{P[Y = 1 | \mathbf{X} = \mathbf{x}]}{P[Y = 0 | \mathbf{X} = \mathbf{x}]}
$$

- The logistic regression equation guarantees that a value between 0 and 1 is calculated. This is evident the when the inverse logit transformation is applied, which results in a "direct" probability prediction.

$$
p(\mathbf{x_i}) = P[Y = 1 | \mathbf{X} = \mathbf{x}] = \frac{\exp^{\beta_0 + \beta_1x_1 + ... + \beta_{p-1}x_{i(p-1)}}}{1 + \exp^{\beta_0 + \beta_1x_1 + ... + \beta_{p-1}x_{i(p-1)}}}
$$

- Note that this is prediction of "probability", not a numerical value. This probability value must be translated to a categorical prediction.

##

We can recover the odds by exponentiating the log-odds:

$$
{\displaystyle {\frac {p}{1-p}}=b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}}.
$$

By simple algebraic manipulation, the probability that ${\displaystyle Y=1}$ is

$$
{\displaystyle p={\frac {b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}}{b^{\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}+1}}={\frac {1}{1+b^{-(\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2})}}}}
$$


The above formula shows that once ${\displaystyle \beta _{i}}$ are fixed, we can easily compute either the log-odds that ${\displaystyle Y=1}$ for a given observation, or the probability that ${\displaystyle Y=1}$ for a given observation. The main use-case of a logistic model is to be given an observation ${\displaystyle (x_{1},x_{2})}$, and estimate the probability ${\displaystyle p}$ that ${\displaystyle Y=1}$. In most applications, the base ${\displaystyle b}$ of the logarithm is usually taken to be `e`.

## Example: Inerpreting logarithm of odds

<!-- Use ISLR dataset `ISLR::Default`; https://rpubs.com/aelhabr/logistic-regression-tutorial -->
We consider an example with ${\displaystyle b=10}$, and coefficients ${\displaystyle \beta _{0}=-3}$ ${\displaystyle \beta _{1}=1}$  ${\displaystyle \beta _{2}=2}$. To be concrete, the model is

$$
{\displaystyle \log _{10}{\frac {p}{1-p}}=\ell =-3+x_{1}+2x_{2}}
$$

where ${\displaystyle p}$ is the probability of the event that ${\displaystyle Y=1}$.

##

- This can be interpreted as follows:
  - ${\displaystyle \beta _{0}=-3}$ is the y-intercept. It is the log-odds of the event that ${\displaystyle Y=1}$, when the predictors ${\displaystyle x_{1}=x_{2}=0}$. By exponentiating, we can see that when ${\displaystyle x_{1}=x_{2}=0}$ the odds of the event that ${\displaystyle Y=1}$ are 1-to-1000, or ${\displaystyle 10^{-3}}$ . Similarly, the probability of the event that ${\displaystyle Y=1}$ when ${\displaystyle x_{1}=x_{2}=0}$ can be computed as ${\displaystyle 1/(1000+1)=1/1001}$.
  - ${\displaystyle \beta _{1}=1}$ means that increasing ${\displaystyle x_{1}}$ by 1 increases the log-odds by ${\displaystyle 1}$. So if ${\displaystyle x_{1}}$ increases by 1, the odds that ${\displaystyle Y=1}$ increase by a factor of ${\displaystyle 10^{1}}$.
  - ${\displaystyle \beta _{2}=2}$ means that increasing ${\displaystyle x_{2}}$ by 1 increases the log-odds by ${\displaystyle 2}$. So if ${\displaystyle x_{2}}$ increases by 1, the odds that ${\displaystyle Y=1}$ increase by a factor of ${\displaystyle 10^{2}.}$ Note how the effect of ${\displaystyle x_{2}}$ on the log-odds is twice as great as the effect of ${\displaystyle x_{1}}$, but the effect on the odds is 10 times greater.
- In order to estimate the parameters ${\displaystyle \beta _{i}}$ from data, one must do logistic regression.

## Example: Probability of passing an exam versus hours of study

- A group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam?

- The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by "1" and "0", are not cardinal numbers. If the problem was changed so that pass/fail was replaced with the grade 0â€“100 (cardinal numbers), then simple regression analysis could be used.

##

- The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).

```{r study-hrs-data}
study_hrs <- tribble(~"Hours", ~"Pass",
        0.50, 0,
        0.75, 0,
        1.0, 0,
        1.25, 0,
        1.50, 0,
        1.75, 0,
        1.75, 1,
        2.0, 0,
        2.25, 1,
        2.50, 0,
        2.75, 1,
        3.0, 0,
        3.25, 1,
        3.50, 0,
        4.0, 1,
        4.25, 1,
        4.5, 1,
        4.75, 1,
        5.0, 1,
        5.5, 1)

study_hrs %>%
  dplyr::mutate(splitgrp = rep(1:2, each = nrow(.) %/% 2 )) %>%
  group_split(splitgrp) %>%
  map(.f = ~ .x %>% dplyr::select(-splitgrp)) %>%
  bind_cols() %>%
  knitr::kable(booktabs = TRUE) %>%
  kableExtra::kable_styling(position = "center", font_size = 6, 
                            latex_options = "striped")
```

##
\small

\begin{columns}[T,onlytextwidth]
  \column{.6\linewidth}

The graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.

```{r log-reg-curve, fig.align='center', fig.width=5, fig.height=3, out.width="80%"}
study_hrs %>% 
  ggplot(aes(Hours, Pass)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE,
              method.args = list(family = "binomial")) +
  theme(axis.text = element_text(size = 12), 
        axis.title = element_text(size = 14))
```

  \column{.4\linewidth}

The logistic regression gives the following output:

```{r glm-fit-and-output}
# # how many categories are there ?
# study_hrs %>% 
#   count(Pass)

# # how many distinct values of predictors are there ?
# study_hrs %>% 
#   distinct(Hours)

# fit the model
study_binmod <- glm(formula = Pass ~ Hours, data = study_hrs, family = "binomial")

# save coefficients
# the output gives a p-value based on the Wald z-score.
# The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution [Wiki](https://en.wikipedia.org/wiki/Logistic_regression).
# an alternative to calculate p-value for logistic-regression is using LRT
study_binmod_coef <- study_binmod %>% tidy()
study_binmod_coef %>% 
  kable(booktabs = TRUE) %>% 
  kableExtra::kable_styling(position = "center", latex_options = "scale_down")

# let us make a scenario where students study for following number of hours
number_hrs_study <- data.frame(Hours = c(seq(0, 10, by = 1), 2.71))
```

\end{columns}

##

The output indicates that hours studying is significantly associated with the probability of passing the exam ( ${\displaystyle p=0.0167}$, Wald test). The output also provides the coefficients for ${\displaystyle {\text{Intercept}}=-4.0777}$ and ${\displaystyle {\text{Hours}}=1.5046}$. The coefficient for Hours values indicate change in the log odds of passing the exam due to one unit change in Hours (i.e. $\beta_1$ coefficient).  These coefficients are entered in the logistic regression equation to estimate the odds (probability) of passing the exam:

$$
{\displaystyle {\begin{aligned}{\text{Log-odds of passing exam}}&=1.5046\cdot {\text{Hours}}-4.0777=1.5046\cdot ({\text{Hours}}-2.71)\\{\text{Odds of passing exam}}&=\exp \left(1.5046\cdot {\text{Hours}}-4.0777\right)=\exp \left(1.5046\cdot ({\text{Hours}}-2.71)\right)\\{\text{Probability of passing exam}}&={\frac {1}{1+\exp \left(-\left(1.5046\cdot {\text{Hours}}-4.0777\right)\right)}}\end{aligned}}}
$$

##

- One additional hour of study is estimated to increase log-odds of passing by `r coef(study_binmod)[2]`, so multiplying odds of passing by  ${\displaystyle \exp(1.5046)\approx 4.5.}$ The form with the x-intercept (2.71) shows that this estimates even odds (log-odds 0, odds 1, probability 1/2) for a student who studies 2.71 hours.

- For example, for a student who studies 2 hours, entering the value ${\displaystyle {\text{Hours}}=2}$ in the equation gives the estimated probability of passing the exam of 0.26:

$${\displaystyle {\text{Probability of passing exam}}={\frac {1}{1+\exp \left(-\left(1.5046\cdot 2-4.0777\right)\right)}}=0.26}$$

- Similarly, for a student who studies 4 hours, the estimated probability of passing the exam is 0.87:

$${\displaystyle {\text{Probability of passing exam}}={\frac {1}{1+\exp \left(-\left(1.5046\cdot 4-4.0777\right)\right)}}=0.87}$$

##

This table shows the probability of passing the exam for several values of hours studying.

```{r manual-calculation-odds-probs, eval=FALSE}
# # then, based on our model
# log odds of passing exam
study_binmod_coef$estimate[2]*number_hrs_study$Hours - study_binmod_coef$estimate[1]

# even odds (or just "odds") of passing becomes one (log odds = 0) when the study hours is 2.71 among the following set of study hours
exp(study_binmod_coef$estimate[2]*(number_hrs_study$Hours-2.71))

# probability of passing exam
1/(1 + exp(-(study_binmod_coef$estimate[2]*number_hrs_study$Hours - study_binmod_coef$estimate[1])))

# # predict with a new data
# # use predict function to get fits for equally spaced values of hours
# predict.glm(study_binmod,
#             number_hrs_study,
#             type = "response")
```


```{r glm-output-printing}

# probability, odds, log odds
# the "link" type gives prediction in terms of log odds, 
# the "response" type gives prediction in terms of probabilities
study_augmented <- map(list("response", "link") %>% # list form is useful when using map_df with .id
                         set_names(.), ~ augment(study_binmod, 
                                                 newdata = number_hrs_study, 
                                                 type.predict = .x), .id = "Type") %>% 
  reduce_right(full_join, by = c("Hours"), suffix = c("_log_odds", "_p_value")) %>% 
  dplyr::select(-contains(".se.fit")) %>% 
  mutate(.fitted_odds = format(round(exp(.fitted_log_odds), 2), scientific = FALSE)) %>% 
  rename_all(list(~str_replace(., ".fitted_", "")))

study_augmented %>% 
  knitr::kable(booktabs = TRUE) %>%
  kableExtra::kable_styling(position = "center", font_size = 6, latex_options = "striped")
```

##

The output from the logistic regression analysis gives a p-value of ${\displaystyle p=0.0167}$, which is based on the Wald z-score. Rather than the Wald method, the recommended method to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for this data gives ${\displaystyle p=0.0006}$.

## Example: Proportions of female children at various ages during adolescence who have reached menarche

The coefficient returned by a logistic regression in r is a logit, or the log of the odds. To convert logits to odds ratio, you can exponentiate it, as you've done above. To convert logits to probabilities, you can use the function `exp(logit)/(1+exp(logit))`. However, there are some things to note about this procedure.

First, lets generate some reproducible data to illustrate. We fit a generalized linear model to menarche dataset with response variable as function of Age. The coefficients displayed are for logits, just as in your example. The `predict()` gives the predicted value in terms of logits.

##

```{r binomial-probability-fit}
library('MASS')
data("menarche")
m<-glm(cbind(Menarche, Total-Menarche) ~ Age, family=binomial, data=menarche)
# summary(m)
broom::tidy(m) %>% 
  knitr::kable(booktabs = TRUE) %>%
  kableExtra::kable_styling(position = "center", font_size = 6, latex_options = "striped")
```

If we plot these data and this model, we see the sigmoidal function that is characteristic of a logistic model fit to binomial data.

```{r binomial-probability-predict}
plot.dat <- data.frame(prob = menarche$Menarche/menarche$Total,
                       age = menarche$Age,
                       fit = predict(m, menarche))
```

```{r binomial-probability-plot, fig.align='center'}
#convert those logit values to probabilities
plot.dat$fit_prob <- exp(plot.dat$fit)/(1+exp(plot.dat$fit))

ggplot(plot.dat, aes(x=age, y=prob)) +
  geom_point() +
  geom_line(aes(x=age, y=fit_prob))
```

##

Note that the change in probabilities is not constant - the curve rises slowly at first, then more quickly in the middle, then levels out at the end. The difference in probabilities between 10 and 12 is far less than the difference in probabilities between 12 and 14. This means that it's impossible to summarise the relationship of age and probabilities with one number without transforming probabilities.

To answer the specific questions: How do you interpret odds ratios?

The odds ratio for the value of the intercept is the odds of a "success" (in your data, this is the odds of taking the product) when x = 0 (i.e. zero thoughts). The odds ratio for your coefficient is the increase in odds above this value of the intercept when you add one whole x value (i.e. x=1; one thought). Using the menarche data:

```{r menarche-coef-interpret}
exp(coef(m)) %>% 
  broom::tidy() %>% 
  knitr::kable(booktabs = TRUE) %>%
  kableExtra::kable_styling(position = "center", font_size = 6, latex_options = "striped")
```

##

We could interpret this as the odds of menarche occurring at age = 0 is .00000000006. Or, basically impossible. Exponentiating the age coefficient tells us the expected increase in the odds of menarche for each unit of age. In this case, it's just over a quintupling. An odds ratio of 1 indicates no change, whereas an odds ratio of 2 indicates a doubling, etc.

Your odds ratio of 2.07 implies that a 1 unit increase in 'Thoughts' increases the odds of taking the product by a factor of 2.07.

How do you convert odds ratios of thoughts to an estimated probability of decision?

You need to do this for selected values of thoughts, because, as you can see in the plot above, the change is not constant across the range of x values. If you want the probability of some value for thoughts, get the answer as follows:

`exp(intercept + coef*THOUGHT_Value)/(1+(exp(intercept+coef*THOUGHT_Value)))`

##

For another example refer to: Mapping QTL in populations with known pedigrees in @griffiths2015introduction (pp 742). 

# Bibliography

## References
